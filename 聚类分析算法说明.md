## 聚类分析

聚类分析为无监督学习方法，其基本思想是采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度就越大。

可参考的博客文：

（1）https://www.cnblogs.com/yuluoxingkong/p/8599717.html



### 一、距离的度量

关于不用距离的计算及应用场景，[可以点击这](https://my.oschina.net/hunglish/blog/787596)

#### 1、欧氏距离

n维空间点`a(x11,x12,…,x1n)`与`b(x21,x22,…,x2n)`间的欧氏距离（两个n维向量）：
$$
d = \sqrt{\sum_{k=1}^ｎ(x_{1k}-x_{2k})^2}
$$

#### 2、曼哈顿距离

$$
d = \sum_{k=1}^ｎ|x_{1k}-x_{2k}|
$$

#### 3、马氏距离

马氏距离的提出：

​				![马氏距离直观解释](assets/马氏距离直观解释.png)	

上图有两个正态分布的总体，它们的均值分别为a和b，但方差不一样，则图中的A点离哪个总体更近？或者说A有更大的概率属于谁？显然，A离左边的更近，A属于左边总体的概率更大，尽管A与a的欧式距离远一些。这就是马氏距离的直观解释。

向量`Xi`与`Xj`之间的马氏距离定义为：
$$
D(X_i,X_j) = \sqrt{(X_i,X_j)^T{S^{-1}}(X_i,X_j)}
$$


​			

### 二、KMeans聚类

不断的迭代最终收敛到中心点到组内距离最小。

#### 1、原理

输入：样本数据集DD，聚类簇数k 

（1）从样本中随机选取k个样本点作为初始的均值向量{μ1,μ2,⋯,μk} ；

（2）循环以下几步直到达到停止条件（收敛条件）： 

- 令Ci=∅(1≤i≤k) ；

-  对所有样本点计算他们到k个均值向量之间的距离，取其中距离最短的距离对应的均值向量的标记作为该点的簇标记，然后将该点加入相应的簇Ci；

- 对每一个簇计算他们新的均值向量![img](C:\Users\GuoKangli\Desktop\层次聚类\assets\clip_image001.png)，如果相比之前的向量有变化，就更新，将其作为新的均值向量，如果没有变化就不变

#### 2、K值的确定

[最优K值的选取](https://blog.csdn.net/qq_15738501/article/details/79036255)

##### （1）手肘法

随着聚类数k的增大，样本划分会更加精细，每个簇的聚合程度会逐渐提高，那么误差平方和SSE自然会逐渐变小。并且，当k小于最佳聚类数时，由于k的增大会大幅增加每个簇的聚合程度，故SSE的下降幅度会很大，而当k到达最佳聚类数时，再增加k所得到的聚合程度回报会迅速变小，所以SSE的下降幅度会骤减，然后随着k值的继续增大而趋于平缓，也就是说SSE和k的关系图是一个手肘的形状，而这个肘部对应的k值就是数据的最佳聚类数。这也是该方法被称为手肘法的原因。
原文链接：https://blog.csdn.net/harry_128/article/details/80523568



### 三、层次聚类

#### 1、原理

先计算样本之间的距离，每次将距离最近的点合并到同一个类；然后，再计算类与类之间的距离，将距离最近的类合并为一个大类。

#### 2、计算类与类之间的方法

（1）最短距离法（single）：将类与类的距离定义为类与类之间样本的最短距离；
$$
d(u,v) = min(dist(u[i],v[j])
$$
其中：u，v为类；i为u类中的点；j为v类中的点。

![最小距离方法](assets/最小距离方法.png)



（2）最长距离法（complete）：将类与类的距离定义为类与类之间样本的最长距离；
$$
d(u,v) = max(dist(u[i],v[j])
$$
其中：u，v为类；i为u类中的点；j为v类中的点。

![最长距离方法](assets/最长距离方法.png)

（3）均值距离法（average）:  计算两个组合数据点中的每个数据点与其他所有数据点的距离，将所有距离的均值作为两个组合数据点间的距离（非加权）；
$$
d(u,v) = \sum_{ij} \frac{d(u[i],v[j])}{(|u|*|v|)}
$$
其中：|u|，|v|是聚类u和v中元素的的个数。

![均值距离方法](assets/均值距离方法.png)

（4）weighted距离法：难以用图和文字说明，直接看公式吧，它和均值距离法得区别可参见下面实际应用中得例子；
$$
d(u,v) = (dist(s,v) + dist(t,v))/2
$$
其中：u是由s和t形成的，而v是森林中剩余的聚类簇，这被称为WPGMA（加权分组平均）法。

 （5）ward方法（沃德方差最小化算法）

![均值距离方法](assets/ward距离方法.png)

- 具体解释如下（摘自[CSDN](https://blog.csdn.net/waltertan1988/article/details/73250691)）：

Ⅰ输入距离矩阵，初始化每一个点为cluster，此时每个组内的ESS为０，ESS公式如下：
$$
ESS = \sum_{ij}^ｎx_ｉ^2－\frac{１}{ｎ}\left(\sum_{ij}^ｎx_ｉ^2\right)^2
＝nVar(X)=nE[(X-E(X))^2]
$$
Ⅱ 计算合个cluster的成本：

```
cost = ESS（总-合并后）-ESS（总-合并前）
ESS（总-合并前）=ESS（红）+ESS（黄）+ESS（其他没画出来的组）
ESS（总-合并后）=ESS（红黄）+ESS（其他没画出来的组）
```

画的那个树状图的高度，可以认为是上面说的这个“成本”。

- 其中在`python scipy.cluster.hierarchy`算法中用到的目标函数如下（虽然我能理解上面说的成本，但是对于具体算法中用的这：

$$
d(u,v) = \sqrt{\frac{|v|+|s|}{T}{d(v,s)^2}+\frac{|v|+|t|}{T}{d(v,t)^2}-\frac{|v|}{T}{d(s,t)^2}}
$$
u是s和t组成的新的聚类，v是森林中未使用的聚类。T = |v|+|s|+|t|，|*|是聚类簇中观测值的个数。在下一章节中会有具体的例子来说明这一公式。

#### 3、实际应用

假设一样本数据（距离矩阵）如下，根据不同计算距离的方法画出层次聚类图：

![样本数据](assets/样本数据.png)

##### （1）最短距离法

Ⅰ两两之间b与c之间的距离最小，先聚合b和c，并重新计算距离距离、更新矩阵：

```
例如层u(b,c)与a的距离为：
d(u(b,c),a)=min(d(b,a),d(c,a))=min(21.6,22.6)=21.6
```

![最短距离法-更新矩阵](assets/最短距离法-更新矩阵.png)

Ⅱ 基于新的距离矩阵，d和e之间的距离最小，聚合d和e，再次更新距离矩阵；

Ⅲ 重复以上步骤，知道所有的样本都在一个类中，最后画出层次聚类图。

![最短距离法-层次聚类图](assets/最短距离法-层次聚类图.png)

##### （2）均值距离法

Ⅰ 两两之间b与c之间的距离最小，先聚合b和c，并重新计算距离距离、更新矩阵：

```
例如在第三次更新距离矩阵时层u(b,c,f)与a的距离为：
d(u(b,c,f),a)=sum(d(b,a),d(c,a),d(f,a))/3=(21.6+22.6+17.7)=20.633
```

![均值距离法-更新矩阵](assets/均值距离法-更新矩阵.png)

Ⅱ 基于新的距离矩阵，d和e之间的距离最小，聚合d和e，再次更新距离矩阵；

Ⅲ 重复以上步骤，知道所有的样本都在一个类中，最后画出层次聚类图。

![均值距离法-层次聚类图](assets/均值距离法-层次聚类图.png)

##### （3）weighted距离法

步骤均与以上两种方法相同。通过相同的例子来说明和均值距离法得差别：

```
例如在第三次更新距离矩阵时层u(b,c,f)与a的距离为【采用第二次更新后的矩阵】：
d(u(b,c,f),a)=sum(d(u(b,c),f),d(f,a))/2=(21.1+17.7)=19.9
```

![weighted方法法-更新矩阵](assets/weighted方法-更新矩阵.png)

![weighted方法-层次聚类图](assets/weighted方法-层次聚类图.png)

##### （4）ward距离法

下面以第三次更新矩阵后为例，计算层`u(d,e)`与`u(b,c,f)`之间的距离：

```
- 根据公式中提到的u是s和t组成的新的聚类，v是森林中未使用的聚类。
- 在该例中新的聚类u极为u(b,c,f),s为u(b,c),v为f。
- T = |v|+|s|+|t|=2+1+2=5
```

因此公式为：
$$
d(u(b,c,f),u(b,c)) = \sqrt{\frac{4}{5}{d(u(b,c),u(d,e))^2}+\frac{3}{5}{d(f,u(d,e))^2}-\frac{2}{5}{d(f,u(b,c))^2}} = \sqrt{\frac{4}{5}{59.93^2}+\frac{3}{5}{54.0^2}-\frac{2}{5}{9.07^2}}=67.76
$$


![ward距离法-更新矩阵](assets/ward距离法-更新矩阵.png)

![ward距离法-层次聚类图](assets/ward距离法-层次聚类图.png)



